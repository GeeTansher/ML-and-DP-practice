{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "# !pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# PATH = os.path.join('Practice', 'images')\n",
    "# os.makedirs(PATH)\n",
    "cascade_classifier = cv2.CascadeClassifier('D:\\\\ML-and-DP-practice\\\\Machine and Deep Learning\\\\Practice\\\\haarcascades\\\\haarcascade_frontalface_default.xml')\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret,frame=cap.read()\n",
    "    gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    detection = cascade_classifier.detectMultiScale(gray,1.3,5)\n",
    "#     cv2.imshow('frame',frame)\n",
    "    \n",
    "    if(len(detection)>0):\n",
    "        (x,y,w,h)=detection[0]        \n",
    "        frame = cv2.rectangle(frame,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "        img_crop=frame[y:y+h,x:x+w]\n",
    "        img_crop=cv2.resize(img_crop,(255,255))\n",
    "        \n",
    "    cv2.imshow('frame',frame)\n",
    "        \n",
    "    if cv2.waitKey(1) & 0XFF == ord('a'):\n",
    "        cv2.imwrite(\"images/\"+str(1)+\".jpg\",img_crop)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0XFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mtcnn in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: opencv-python>=4.1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from mtcnn) (4.6.0.66)\n",
      "Requirement already satisfied: keras>=2.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from mtcnn) (2.9.0)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from opencv-python>=4.1.0->mtcnn) (1.22.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install mtcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mtcnn.mtcnn import MTCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('D:\\\\ML-and-DP-practice\\\\Machine and Deep Learning\\\\Practice\\\\images\\\\1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  0, 255,   0],\n",
       "        [  0, 255,   1],\n",
       "        [  2, 253,   0],\n",
       "        ...,\n",
       "        [  6, 255,   7],\n",
       "        [  4, 255,   4],\n",
       "        [  4, 254,   0]],\n",
       "\n",
       "       [[  0, 253,   0],\n",
       "        [ 12, 254,  11],\n",
       "        [ 25, 240,  19],\n",
       "        ...,\n",
       "        [ 28, 230,  29],\n",
       "        [ 16, 233,  18],\n",
       "        [ 18, 255,  15]],\n",
       "\n",
       "       [[  7, 255,   2],\n",
       "        [ 23, 238,  17],\n",
       "        [ 25, 168,  12],\n",
       "        ...,\n",
       "        [  0, 102,   0],\n",
       "        [  0, 139,   0],\n",
       "        [ 21, 239,  20]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[  0, 255,   0],\n",
       "        [ 30, 233,  32],\n",
       "        [120, 216, 125],\n",
       "        ...,\n",
       "        [145, 170, 160],\n",
       "        [107, 195, 119],\n",
       "        [ 32, 230,  34]],\n",
       "\n",
       "       [[  0, 255,   0],\n",
       "        [ 30, 233,  32],\n",
       "        [120, 216, 125],\n",
       "        ...,\n",
       "        [142, 169, 159],\n",
       "        [106, 194, 118],\n",
       "        [ 32, 230,  34]],\n",
       "\n",
       "       [[  0, 255,   0],\n",
       "        [ 30, 234,  31],\n",
       "        [120, 216, 125],\n",
       "        ...,\n",
       "        [142, 169, 159],\n",
       "        [104, 194, 118],\n",
       "        [ 32, 230,  34]]], dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 360ms/step\n",
      "1/1 [==============================] - 0s 312ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 241ms/step\n",
      "WARNING:tensorflow:5 out of the last 702 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001C7779E0A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 243ms/step\n",
      "{'box': [24, 0, 206, 264], 'confidence': 0.9999567270278931, 'keypoints': {'left_eye': (82, 99), 'right_eye': (180, 103), 'nose': (128, 155), 'mouth_left': (81, 198), 'mouth_right': (174, 203)}}\n"
     ]
    }
   ],
   "source": [
    "detector = MTCNN()\n",
    "faces = detector.detect_faces(image)\n",
    "for face in faces:\n",
    "    print(face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bbox(image):\n",
    "    faces =detector.detect_faces(image)\n",
    "    bounding_box = faces[0]['box']\n",
    "    keypoints = faces[0]['keypoints']\n",
    "    confidence = faces[0]['confidence']\n",
    "    \n",
    "    cv2.rectangle(image,(bounding_box[0], bounding_box[1]),\n",
    "                  (bounding_box[0]+bounding_box[2],bounding_box[1]+bounding_box[3]),\n",
    "                    (0,0,255),2)\n",
    "    \n",
    "    cv2.circle(image, (keypoints['left_eye']), 2, (0,255,255), 2)\n",
    "    cv2.circle(image, (keypoints['right_eye']), 2, (0,255,255), 2)\n",
    "    cv2.circle(image, (keypoints['nose']), 2, (0,255,255), 2)\n",
    "    cv2.circle(image, (keypoints['mouth_left']), 2, (0,255,255), 2)\n",
    "    cv2.circle(image, (keypoints['mouth_right']), 2, (0,255,255), 2)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n"
     ]
    }
   ],
   "source": [
    "img = create_bbox(image)\n",
    "cv2.imshow('img',img)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret,frame=cap.read()\n",
    "    image = cv2.resize(frame, (frame.shape[1] // size, frame.shape[0] // size))\n",
    "    image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "    faces =detector.detect_faces(image)\n",
    "    for face in faces:\n",
    "        # bounding_box = face['box']\n",
    "        x, y, w, h = [v * size for v in face['box']]\n",
    "        # face_img = frame[y:y+h, x:x+w]\n",
    "        # resized = cv2.resize(face_img, ())\n",
    "   \n",
    "        cv2.rectangle(image,(x, y),(x+w,y+h), (0,0,255),2)\n",
    "        \n",
    "        # img_crop=frame[y:y+h,x:x+w]\n",
    "        # img_crop=cv2.resize(img_crop,(255,255))\n",
    "        \n",
    "    cv2.imshow('frame',frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0XFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "f6246b25e200e4c5124e3e61789ac81350562f0761bbcf92ad9e48654207659c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
